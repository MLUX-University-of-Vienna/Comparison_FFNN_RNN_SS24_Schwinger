{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras_tuner as kt\n",
    "import category_encoders\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns_with_a_single_value(original_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes all columns that only contain one possible value and therefore do not provide any useful information for the neural network.\n",
    "\n",
    "    Columns with a single unique do not provide any information for the neural network, since every possible sample will have the same value. \n",
    "\n",
    "    Args:\n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to check for columns with single unique values.\n",
    "\n",
    "    Returns: \n",
    "        pd.DataFrame: A modified version of the input DataFrame.\n",
    "        Note: All columns with a single unique value are dropped.\n",
    "    \"\"\"\n",
    "    return original_dataframe[[column for column in original_dataframe.columns if len(original_dataframe[column].unique()) > 1]]\n",
    "\n",
    "def remove_low_appearance_values(original_dataframe: pd.DataFrame, count_threshold: int, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates how often each unique values appears in a specific column of the {original_dataframe}.\n",
    "    \n",
    "    Rows in the {original_dataframe} with unique values appearing below the {count_threshold} will get removed from the DataFrame. \n",
    "\n",
    "    Args:   \n",
    "        original_dataframe (pd.DataFrame): The input dataframe to be modified.\n",
    "        count_threshold (int): All unique values with a value_count below this number will be removed.\n",
    "        column_name (str): The name of the column in the DataFrame on which the value_count for each unique value is calculated.\n",
    "\n",
    "    Return:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with all unique values appearing below the defined threshold in {column_name} removed.\n",
    "    \"\"\"\n",
    "    appearance_count = original_dataframe[column_name].value_counts()\n",
    "    return original_dataframe[~original_dataframe[column_name].isin(appearance_count[appearance_count < count_threshold].index.tolist())]\n",
    "\n",
    "def remove_high_appearance_values(original_dataframe: pd.DataFrame, count_threshold: int, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates how often each unique values appears in a specific column of the {original_dataframe}.\n",
    "    \n",
    "    Rows in the {original_dataframe} with unique values appearing more than the {count_threshold} will get removed from the DataFrame as they can be considered as outliers. \n",
    "\n",
    "    Args:   \n",
    "        original_dataframe (pd.DataFrame): The input dataframe to be modified.\n",
    "        count_threshold (int): All unique values with a value_count higher than this number will be removed.\n",
    "        column_name (str): The name of the column in the DataFrame on which the value_count for each unique value is calculated.\n",
    "\n",
    "    Return:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with all unique values appearing more often than the defined threshold in {column_name} removed.\n",
    "    \"\"\"\n",
    "    appearance_count = original_dataframe[column_name].value_counts()\n",
    "    return original_dataframe[~original_dataframe[column_name].isin(appearance_count[appearance_count > count_threshold].index.tolist())]\n",
    "\n",
    "def number_of_unique_values_per_column(dataframe: pd.DataFrame) -> list[(str, int)]:\n",
    "    \"\"\" \n",
    "    Returns a list of tuples containing column names and the amount of unique values in this column.\n",
    "\n",
    "    Args:  \n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to determine the number of unique values per column.\n",
    "\n",
    "    Returns:\n",
    "        list[(str, int)]: A list of tuples where each tuple contains:\n",
    "            - The column name as a string.\n",
    "            - The amount of unique values in that column as an integer.\n",
    "    \"\"\"\n",
    "    return [(column, dataframe[column].nunique()) for column in dataframe.columns]\n",
    "\n",
    "def replace_null_values_with_column_mode(original_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Returns the DataFrame without any null value. \n",
    "    Each null value gets substituted with the mode value of the column.\n",
    "    The mode is the value that appears with the highest frequency in the column.\n",
    "    The null values get substituted, because a neural network needs non-null integer values as input features. \n",
    "\n",
    "    Args:\n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to modify.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input DataFrame without null values.\n",
    "    \"\"\"\n",
    "    original_dataframe = original_dataframe.infer_objects(copy=False)\n",
    "    null_columns = original_dataframe.columns[original_dataframe.isnull().any()]\n",
    "    \n",
    "    for column in null_columns:\n",
    "        original_dataframe[column] = original_dataframe[column].fillna(original_dataframe[column].mode()[0])\n",
    "    return original_dataframe\n",
    "\n",
    "def split_arrays(original_array: np.array) -> tuple[np.array, np.array]:\n",
    "    \"\"\" \n",
    "    Returns a tuple of np.arrays.\n",
    "\n",
    "    Parameters:\n",
    "        original_array (np.array): The input train or test array to modify and slice the first column. \n",
    "\n",
    "    Returns:\n",
    "        tuple[np.array, np.array]: A tuple of two np.arrays containing:\n",
    "            - The splitted array with the data for the embedding layer.\n",
    "            - The remaining array with the remaining input features, which are not embedded.        \n",
    "    \"\"\"\n",
    "    return original_array[:, :, :1], original_array[:, :, 1:]\n",
    "\n",
    "def get_embedding_input_dim(array_to_embed: np.array) -> int:\n",
    "    \"\"\" \n",
    "    Returns the necessary size of the input dimension for the embedding layer.\n",
    "    \n",
    "    Parameters:\n",
    "        array_to_embbed (np.array): The input array used for calculation.\n",
    "\n",
    "    Returns:\n",
    "        int: The necessary size of the input dimension for the embedding layer.\n",
    "    \"\"\"\n",
    "    return len(set(array_to_embed.flatten()))\n",
    "\n",
    "def move_column_to_the_front(original_dataframe: pd.DataFrame, column_to_move: str) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Returns the DataFrame with the {column_to_move} moved at the first index of the DataFrame columns.\n",
    "\n",
    "    Args:\n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to modify.\n",
    "        column_to_move (str): The name of the column to move to the beginning of the input DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with the {column_to_move} at the beginning of the DataFrame.\n",
    "    \"\"\"    \n",
    "    first_column = original_dataframe.pop(column_to_move)\n",
    "    original_dataframe.insert(0, column_to_move, first_column)\n",
    "    return original_dataframe\n",
    "\n",
    "def get_weather_data(original_dataframe: pd.DataFrame, column_to_encode: str, json_subarray_name: str) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Returns the DataFrame with more detailed weather data. \n",
    "\n",
    "    Args: \n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to modify.\n",
    "        column_to_encode (str):  The name of the column from the input DataFrame with the ID to fetch the weather data.\n",
    "        json_subarray_name (str): The name of the subarray in the json that contains the in-depth weather data for the ID included in the input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with the weather_data included.\n",
    "        Note: All columns with only a single unique value get removed and the column with the ID ({column_to_encode}) also gets dropped. \n",
    "    \"\"\"\n",
    "    weather_data_list = []\n",
    "    \n",
    "    for _, row in original_dataframe.iterrows():\n",
    "        # get the id from the original dataframe\n",
    "        weather_id = row[column_to_encode]\n",
    "        # get the enum dict corresponding to the id\n",
    "        weather_day_data = parsed_json[json_subarray_name][str(weather_id)]\n",
    "        # add the {column to encode} to the key since weather_day and weather_hour share keys\n",
    "        weather_day_data = {f\"{column_to_encode} {k}\": v for k, v in weather_day_data.items() if k != 'id'}\n",
    "        weather_data_list.append(weather_day_data)\n",
    "    weather_data_df = pd.DataFrame(weather_data_list, index=original_dataframe.index)\n",
    "    weather_data_df = remove_columns_with_a_single_value(weather_data_df)\n",
    "    original_dataframe = pd.concat([original_dataframe, weather_data_df], axis=1)\n",
    "\n",
    "    # drop the id column since we already fetched the data and do not need it anymore\n",
    "    original_dataframe = original_dataframe.drop(columns=column_to_encode)\n",
    "\n",
    "    return original_dataframe\n",
    "\n",
    "def build_feature_vectors(grouped_df: pd.DataFrame, classification_column_name: str) -> list[list[tuple[pd.Series, float]]]:\n",
    "    \"\"\"\n",
    "    Returns the input_feature_vectors and their classifications in form of a tuple containing the input feature vector as pd.Series and the classification as float.\n",
    "\n",
    "    Args:\n",
    "        grouped_df (df.DataFrame): The grouped input modify to build the feature_vectors.\n",
    "        classification_column_name (str): The name of the column of the {grouped_df} that contains the classification. \n",
    "    \n",
    "    Returns:\n",
    "        list[list[tuple[pd.Series, float]]]: A list of sublist containing tuples of a pd.Series and a float value:\n",
    "            - The input feature vector as pd.Series from the pd.DataFrame.\n",
    "            - The classification of the input feature vector as float value.\n",
    "    \"\"\"\n",
    "    feature_vectors = []\n",
    "    \n",
    "    for _, group in grouped_df:\n",
    "        feature_vector = []\n",
    "        for index in range(0, len(group)):\n",
    "            if index < len(group) - 1:\n",
    "                feature_vector.append((group.iloc[index], group.iloc[index+1][classification_column_name]))\n",
    "        if(len(feature_vector) > 0):                \n",
    "            feature_vectors.append(feature_vector)\n",
    "    return feature_vectors\n",
    "\n",
    "def split_feature_vectors(feature_vectors: list[list[tuple[pd.Series, float]]]) -> tuple[list[list[pd.Series]], list[list[float]]]:\n",
    "    \"\"\" \n",
    "    Returns two lists. \n",
    "    The first containing the input_feature_vectors.\n",
    "    The other containing the classifications for the input_feature_vectors\n",
    "    \n",
    "    Args:\n",
    "        feature_vectors (list[list[tuple[pd.Series, float]]]):\n",
    "    \n",
    "    Returns:\n",
    "        tuple[list[list[pd.Series]], list[list[float]]]: A tuple of two list containing:\n",
    "            - A list of sublists containing the input_feature_vectors as pd.Series. \n",
    "            - A list of sublists containing the classifications for the input_feature_vectors as float values.\n",
    "    \"\"\"\n",
    "    input_features = []\n",
    "    classification_labels = []\n",
    "\n",
    "    for group in feature_vectors:\n",
    "        group_input_features = []\n",
    "        group_classification_labels = []\n",
    "        for input_feature, classification in group:\n",
    "            group_input_features.append(input_feature)\n",
    "            group_classification_labels.append(classification)\n",
    "        input_features.append(group_input_features)\n",
    "        classification_labels.append(group_classification_labels)\n",
    "    return input_features, classification_labels\n",
    "\n",
    "def pad_input_features(input_features: list[list[float]], sequence_length: int) -> list[list[float]]:\n",
    "    \"\"\" \n",
    "    Returns a multidimensional list of padded input_features.\n",
    "\n",
    "    Args: \n",
    "        input_features (list[list[float]]): A list containing all input features.\n",
    "        sequence_length (int): The previously calculated sequence_length for every feature in every subsession. \n",
    "\n",
    "    Returns:\n",
    "        list[list[float]]: A list of sessions, where each subsession is padded to the desired length.\n",
    "        Note: All existing entries of a session get converted to list[float] from pd.Series.\n",
    "    \"\"\"\n",
    "    padded_list = []\n",
    "    for subsession in input_features:\n",
    "        # Calculate the mode of the current session based on the first session entry\n",
    "        # [0] returns the value, [1] return the count how often the values appears in the list\n",
    "        mode_value = list(stats.mode(subsession)[0])\n",
    "\n",
    "        # convert all pd.Series in the subsession entries to lists\n",
    "        for i in range(len(subsession)):\n",
    "            subsession[i] = subsession[i].tolist()\n",
    "        \n",
    "        # Check if the subsession needs to be padded\n",
    "        if len(subsession) < sequence_length:\n",
    "            range_to_pad = sequence_length - len(subsession)\n",
    "            for _ in range(range_to_pad):\n",
    "                subsession.insert(0, mode_value)\n",
    "        else:\n",
    "            subsession = subsession[:sequence_length]\n",
    "\n",
    "        padded_list.append(subsession)\n",
    "    return padded_list\n",
    "\n",
    "def pad_classifications(list_of_all_classifications: list[float], sequence_length: int) -> list[float]:\n",
    "    \"\"\" \n",
    "    Return a multidimensional list of padded classifications.\n",
    "        \n",
    "    Args: \n",
    "        list_of_all_classifications (list[float]): A list containing all classifications.\n",
    "        sequence_length (int): The previously calculated sequence_length for every subsession.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: A list of sessions, where each subsession is padded to the desired length.\n",
    "        Note: All existing entries of a session get converted to list[float] from pd.Series.\n",
    "    \"\"\"\n",
    "    padded_list = []\n",
    "    for classification_for_subsession in list_of_all_classifications:\n",
    "        # Calculate the mode of the classification for the current subsession\n",
    "        # [0] returns the value, [1] return the count how often the values appears in the list\n",
    "        mode_value = stats.mode(classification_for_subsession)[0]\n",
    "        \n",
    "        # Check if the subsession needs to be padded\n",
    "        if len(classification_for_subsession) < sequence_length:\n",
    "            range_to_pad = sequence_length - len(classification_for_subsession)\n",
    "            for _ in range(range_to_pad):\n",
    "                classification_for_subsession.insert(0, mode_value)\n",
    "        else:\n",
    "            classification_for_subsession = classification_for_subsession[:sequence_length]\n",
    "\n",
    "        padded_list.append(classification_for_subsession)\n",
    "    return padded_list\n",
    "\n",
    "def plot_history(history: dict) -> None:\n",
    "    \"\"\"\n",
    "    Plots the training and validation accuracy and loss over epochs for a training process of a Keras Model.\n",
    "\n",
    "    Parameters:\n",
    "        history (dict): A dictionary object returned by the fit() method of a Keras Model. It contains the training and validation metrics recorded during the training process.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(loc='lower right')\n",
    "\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "\n",
    "    print(\"val_accuracy: \", history.history['val_accuracy'])\n",
    "    print(\"accuracy: \", history.history['accuracy'])\n",
    "    print(\"val_loss: \", history.history['val_loss'])\n",
    "    print(\"loss: \", history.history['loss'])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def encode_cyclic_feature(original_dataframe: pd.DataFrame, column_to_encode: str, period: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encodes a cyclical feature in a DataFrame column using a cosine and sine transformation, \n",
    "    to let the neural network correctly learn periodic patterns.\n",
    "\n",
    "    The original feature (e.g. time, date) gets split up into two new columns.\n",
    "    A cosine and a sine column while the original column gets dropped.\n",
    "    \n",
    "    Args:\n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to modify the cyclical features column. \n",
    "        column_to_encode (str): The name of the column to modify in the input DataFrame.\n",
    "        period (int):  The timeframe of a single cycle in the column to be encoded. \n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with the {column_to_encode} replaced by two new columns:\n",
    "            - {column_to_encode}_sin: The sine-transformed cyclical feature.\n",
    "            - {column_to_encode}_cos: The cosine-transformed cyclical feature.\n",
    "        Note: The {column_to_encode} is dropped from the DataFrame after encoding.\n",
    "    \"\"\"\n",
    "    original_dataframe[column_to_encode] = original_dataframe[column_to_encode].astype('float64')\n",
    "    original_dataframe[column_to_encode + '_sin'] = np.sin(2 * np.pi * original_dataframe[column_to_encode] / period)\n",
    "    original_dataframe[column_to_encode + '_cos'] = np.cos(2 * np.pi * original_dataframe[column_to_encode] / period)\n",
    "    original_dataframe = original_dataframe.drop(columns=column_to_encode)\n",
    "    return original_dataframe\n",
    "\n",
    "def encode_utc_timestamps_with_two_different_patterns(original_dataframe: pd.DataFrame, column_to_encode: str, patterns: list[str]) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Encodes a utc timestamp in a DataFrame column to a unix timestamp, to convert the timestamp into an integer, \n",
    "    that can be used as input feature for the neural network.\n",
    "\n",
    "    Iterates over all entries in the {column_to_encode}, transforms them into an unix timestamp.\n",
    "    These unix timestamps then get added to a list containing all timestamps. \n",
    "    After iterating over all entries the values in the {column_to_encode} get substituted with the list of timestamps. \n",
    "    \n",
    "    Used for columns with two different possible utc patterns.\n",
    "\n",
    "    Args:\n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to encode the utc timestamp.\n",
    "        column_to_encode (str): The name of the column to modify in the input DataFrame.\n",
    "        patterns (list[str]): A list of two different string patterns of the timestamp in the {column_to_encode}:\n",
    "            - The first pattern contains the pattern with a period.\n",
    "            - The second pattern contains the pattern without the period.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with the {column_to_encode} encoded to a unix timestamp.\n",
    "    \"\"\"\n",
    "    timestamps = []\n",
    "    for index, _ in original_dataframe.iterrows():\n",
    "        time_stamp = original_dataframe.loc[index, column_to_encode]\n",
    "        if time_stamp.__contains__(\".\"):\n",
    "            pattern = patterns[0]\n",
    "        else: \n",
    "            pattern = patterns[1]\n",
    "        time_stamp = datetime.strptime(original_dataframe.loc[index, column_to_encode], pattern)\n",
    "        timestamps.append(time_stamp.timestamp())\n",
    "\n",
    "    original_dataframe[column_to_encode] = timestamps\n",
    "    return original_dataframe\n",
    "\n",
    "def encode_utc_timestamp(original_dataframe: pd.DataFrame, column_to_encode: str, pattern: str) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Encodes a utc timestamp in a DataFrame column to a unix timestamp, to convert the timestamp into an integer, \n",
    "    that can be used as input feature for the neural network.\n",
    "\n",
    "    Iterates over all entries in the {column_to_encode}, transforms them into an unix timestamp.\n",
    "    These unix timestamps then get added to a list containing all timestamps. \n",
    "    After iterating over all entries the values in the {column_to_encode} get substituted with the list of timestamps. \n",
    "    \n",
    "    Args:\n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to encode the utc timestamp.\n",
    "        column_to_encode (str): The name of the column to modify in the input DataFrame.\n",
    "        pattern (str): The string pattern of the timestamp in the {column_to_encode}.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with the {column_to_encode} encoded to a unix timestamp.\n",
    "    \"\"\"\n",
    "\n",
    "    timestamps = []\n",
    "    for index, _ in original_dataframe.iterrows():\n",
    "        time_stamp = original_dataframe.loc[index, column_to_encode]\n",
    "        time_stamp = datetime.strptime(original_dataframe.loc[index, column_to_encode], pattern)\n",
    "        timestamps.append(time_stamp.timestamp())\n",
    "\n",
    "    original_dataframe[column_to_encode] = timestamps\n",
    "    return original_dataframe\n",
    "\n",
    "def encode_boolean(original_dataframe: pd.DataFrame, column_to_encode: str) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Transforms the content of column with boolean string representation into a binary integer representation.\n",
    "\n",
    "    Args:\n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to modify.\n",
    "        column_to_encode (str): The name of the column to modify in the input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input with an integer representation of the boolean variable.\n",
    "    \"\"\"\n",
    "    original_dataframe[column_to_encode] = original_dataframe[column_to_encode].astype('int')\n",
    "    return original_dataframe\n",
    "\n",
    "def encode_date(original_dataframe: pd.DataFrame, column_to_encode: str, pattern: str) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Transforms a Date as string with the specified string pattern into a ordinal encoded representation.\n",
    "\n",
    "    Ordinal encoded integers imply an ordering. \n",
    "\n",
    "    Args:\n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to modify.\n",
    "        column_to_encode (str): The name of the column to modify in the input DataFrame.\n",
    "        pattern (str): The string pattern of the date string.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with an ordinal encoded representation of the date variable. \n",
    "    \"\"\"\n",
    "    for _, row in original_dataframe.iterrows():\n",
    "        date_object = datetime.strptime(row[column_to_encode], pattern)\n",
    "        original_dataframe[column_to_encode] = date_object.toordinal()\n",
    "    return original_dataframe\n",
    "\n",
    "def encode_weather_enums(original_dataframe: pd.DataFrame, json_subarray_name: str, json_key: str) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Transforms the string from the weather_hour data into the in the json defined integer encoded representation of the enum.\n",
    "    The same representation is also used in the weather_day data. \n",
    "\n",
    "    Args:\n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to modify.\n",
    "        json_subarray_name (str): The name of the subarray from the json. e.g weather_enums \n",
    "        json_key (str): The name of the array inside the subarray defined in the parameter before. e.g. moon_phase\n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with integer encoded weather data enums.\n",
    "    \"\"\"\n",
    "    # get the enum data from the json file as dict \n",
    "    weather_day_data = parsed_json[json_subarray_name][json_key]\n",
    "    # convert the non-int enum strings to their in the json defined int values\n",
    "    for index, _ in original_dataframe.iterrows():\n",
    "        for key, value in weather_day_data.items():\n",
    "            if original_dataframe.loc[index, f'weather_hour_id {json_key}'] == value:\n",
    "                original_dataframe.loc[index, f'weather_hour_id {json_key}'] = key     \n",
    "    return original_dataframe\n",
    "\n",
    "def encode_weather_data(original_dataframe: pd.DataFrame, weather_id_column: str) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Returns the DataFrame with encoded weather data. \n",
    "\n",
    "    Args: \n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to modify.\n",
    "        weather_id_column (str):  The name of the weather_data to encode. Either weather_day_id or weather_hour_id.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with the weather_data encoded. \n",
    "    \"\"\"\n",
    "\n",
    "    if weather_id_column == \"weather_day_id\":\n",
    "        original_dataframe = encode_utc_timestamp(original_dataframe, f'{weather_id_column} sun_set', \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        original_dataframe = encode_utc_timestamp(original_dataframe, f'{weather_id_column} sun_rise', \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        original_dataframe = encode_utc_timestamp(original_dataframe, f'{weather_id_column} created_at', \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "        original_dataframe = encode_utc_timestamp(original_dataframe, f'{weather_id_column} calculated_at', \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "    elif weather_id_column == \"weather_hour_id\":\n",
    "        original_dataframe = encode_utc_timestamp(original_dataframe, f'{weather_id_column} created_at', \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "        original_dataframe = encode_utc_timestamp(original_dataframe, f'{weather_id_column} calculated_at', \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        original_dataframe = encode_utc_timestamp(original_dataframe, f'{weather_id_column} forecast_time', \"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "    return original_dataframe\n",
    "\n",
    "def onehot_encode_column(original_dataframe: pd.DataFrame, column_to_encode: str) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    One-hot encodes the passed column of the DataFrame. \n",
    "    \n",
    "    One-hot encoding is a technique used to convert categorical variables into a numerical representation,\n",
    "    which is necessary to use nominal non-integer features without an inherent order as input in a neural network.\n",
    "    This guarantees that the neural network can correctly learn from the data without implying any ordinal relationships.\n",
    "\n",
    "    One-hot encoding splits the specified column by creating new binary columns for each unique value present in the column.\n",
    "    Each binary column represents if the corresponding value is present for the row in the DataFrame.\n",
    "\n",
    "    Args:  \n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to modify via one-hot encoding.\n",
    "        column_to_encode (str): The name of the column to modify in the input DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with one-hot encoded columns for the {column_to_encode}\n",
    "        Note: If the {column_to_encode} is not 'session_id', the {column_to_encode} is dropped from the DataFrame after encoding.\n",
    "        The 'session_id' column is still needed later on when looking for the classification for the y_sets. \n",
    "    \"\"\"\n",
    "    encoder = OneHotEncoder()\n",
    "    encoded_column = encoder.fit_transform(original_dataframe[[column_to_encode]])\n",
    "    encoded_column_dataframe = pd.DataFrame(encoded_column.toarray(), columns=encoder.get_feature_names_out([column_to_encode]), index=original_dataframe.index)\n",
    "    original_dataframe = pd.concat([original_dataframe, encoded_column_dataframe], axis=1)\n",
    "\n",
    "    if column_to_encode != \"session_id\":\n",
    "        original_dataframe = original_dataframe.drop(columns=[column_to_encode])\n",
    "\n",
    "    return original_dataframe\n",
    "\n",
    "def label_encode_column(original_dataframe: pd.DataFrame, column_to_encode: str) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Label encodes the passed column of the DataFrame.\n",
    "\n",
    "    Label encoding is a technique used to convert categorical variables into a numerical representation,\n",
    "    which is necessary to use non-integer features as input in a neural network.\n",
    "\n",
    "    Label encoding does transform the categorical variable into a ordinal representation and therefore implies an order. \n",
    "\n",
    "    Args:\n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to modify via label encoding.\n",
    "        column_to_encode (str): The name of the column to modify in the input DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with an integer representation of the categorical variable.\n",
    "    \"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    original_dataframe[column_to_encode] = label_encoder.fit_transform(original_dataframe[column_to_encode])\n",
    "    return original_dataframe\n",
    "\n",
    "def binary_encode_column(original_dataframe: pd.DataFrame, column_to_encode: str) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Binary encodes the passed column of the DataFrame.\n",
    "\n",
    "    Binary encoding is a technique used to convert categorical variables into a binary representation,\n",
    "    which is necessary to use non-integer features as input in a neural network.\n",
    "\n",
    "    Binary encoding first transforms the categorical variables into a numerical representation.\n",
    "    Afterwards this numerical representation is transformed into its binary number and split into multiple columns. \n",
    "    \n",
    "    Binary encoding can imply a weak ordering.\n",
    "\n",
    "    Args:\n",
    "        original_dataframe (pd.DataFrame): The input DataFrame to modify.\n",
    "        column_to_encode (str): The name of the column to modify in the input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A modified version of the input DataFrame with an binary encoded representation of the categorical variable.\n",
    "    \"\"\"\n",
    "    binary_encoder = category_encoders.BinaryEncoder(cols=column_to_encode)\n",
    "    original_dataframe = binary_encoder.fit_transform(original_dataframe)\n",
    "    return original_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is readable\n"
     ]
    }
   ],
   "source": [
    "# read json data\n",
    "path_to_dataset = \"datasets\\\\transfer\\\\smaller_dataset.json\"\n",
    "\n",
    "if os.access(path_to_dataset, os.R_OK):\n",
    "    print(\"File is readable\")\n",
    "    with open(path_to_dataset) as file:\n",
    "        parsed_json = json.load(file)\n",
    "else:\n",
    "    print(\"File is not readable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build DataFrame \n",
    "total_amount_of_rows = 0\n",
    "all_sessions = pd.DataFrame()\n",
    "json_subarray_name = 'traces'\n",
    "for i in pd.json_normalize(parsed_json[json_subarray_name]):\n",
    "    total_amount_of_rows += len(pd.json_normalize(parsed_json[json_subarray_name][i]))\n",
    "    single_session = pd.json_normalize(parsed_json[json_subarray_name][i])\n",
    "    all_sessions = pd.concat([all_sessions, single_session], ignore_index=True)\n",
    "\n",
    "# remove all classifications and sessions that appear below the threshold passed as input parameter\n",
    "all_sessions = remove_low_appearance_values(all_sessions, 5, 'content_id')\n",
    "all_sessions = remove_low_appearance_values(all_sessions, 3, 'session_id')\n",
    "all_sessions = remove_high_appearance_values(all_sessions, 8, 'session_id')\n",
    "\n",
    "# empty columns for the whole dataset\n",
    "all_sessions = all_sessions.drop(columns=['weather_future_day_id', 'weather_future_hour_id'])\n",
    "\n",
    "# replace all null values in the DataFrame with the column mode\n",
    "all_sessions = replace_null_values_with_column_mode(all_sessions)\n",
    "\n",
    "# encode primary features with the weather data\n",
    "all_sessions = encode_cyclic_feature(all_sessions, 'time_hod', 24)\n",
    "all_sessions = encode_cyclic_feature(all_sessions, 'time_dow', 7)\n",
    "all_sessions = encode_utc_timestamps_with_two_different_patterns(all_sessions, 'time_utc', ['%Y-%m-%dT%H:%M:%S.%fZ', '%Y-%m-%dT%H:%M:%SZ'])\n",
    "all_sessions = encode_utc_timestamps_with_two_different_patterns(all_sessions, 'time_local', ['%Y-%m-%dT%H:%M:%S.%f', '%Y-%m-%dT%H:%M:%S'])\n",
    "all_sessions = encode_boolean(all_sessions, 'device_online')\n",
    "all_sessions = label_encode_column(all_sessions, 'content_portal')\n",
    "\n",
    "# get the more in-depth weather data and remove the id to fetch the more in-depth data\n",
    "all_sessions = get_weather_data(all_sessions, 'weather_day_id', 'weather_day_map')\n",
    "all_sessions = get_weather_data(all_sessions, 'weather_hour_id', 'weather_hour_map')\n",
    "\n",
    "# encode previously fetched weather_data\n",
    "all_sessions = encode_weather_data(all_sessions, 'weather_day_id')\n",
    "all_sessions = encode_weather_data(all_sessions, 'weather_hour_id')\n",
    "\n",
    "# absolute data with little relevance or data with too many null values\n",
    "columns_to_drop = ['weather_day_id moon_set', 'weather_day_id moon_rise', 'content_portal', 'device_online', 'time_utc', 'time_local', \n",
    "                   'device_height_px', 'device_width_px', 'weather_day_id sun_set', 'weather_day_id sun_rise', 'weather_day_id moon_set',\n",
    "                   'weather_day_id moon_rise', 'weather_day_id created_at', 'weather_day_id calculated_at', 'weather_day_id forecast_date',\n",
    "                   'weather_hour_id created_at', 'weather_hour_id calculated_at', 'weather_hour_id forecast_time']\n",
    "\n",
    "# remove columns defined earlier\n",
    "all_sessions = all_sessions.drop(columns=columns_to_drop)\n",
    "\n",
    "# encode non int values from the weather data\n",
    "all_sessions = encode_weather_enums(all_sessions, 'weather_enums', 'wind_strength')\n",
    "all_sessions = encode_weather_enums(all_sessions, 'weather_enums', 'wind_direction')\n",
    "all_sessions = encode_weather_enums(all_sessions, 'weather_enums', 'thunderstorm_prob')\n",
    "\n",
    "all_sessions = encode_cyclic_feature(all_sessions, 'weather_hour_id wind_direction', 9)\n",
    "all_sessions = encode_cyclic_feature(all_sessions, 'weather_day_id wind_direction', 9)\n",
    "all_sessions = encode_cyclic_feature(all_sessions, 'weather_day_id moon_phase', 8)\n",
    "\n",
    "# add column names to list depending on the future preproccessing steps\n",
    "ohe_features = ['device_class', 'device_orientation', 'oha_language_iso2', 'oha_layout']\n",
    "\n",
    "binary_features = ['device_country_iso2', 'device_language_iso2', 'event_type', 'device_platform']\n",
    "\n",
    "embedded_features = ['content_id', 'device_id', 'session_id']\n",
    "\n",
    "ordinal_features = ['weather_hour_id thunderstorm_prob', 'weather_day_id thunderstorm_prob', 'weather_hour_id wind_direction_sin', \n",
    "                    'weather_hour_id wind_direction_cos', 'weather_day_id sunshine_h', \n",
    "                    'weather_day_id temp_max_c', 'weather_day_id temp_min_c', 'weather_day_id moon_phase_sin',\n",
    "                    'weather_day_id prec_prob_pct', 'weather_day_id prec_rain_mm_h', 'weather_day_id prec_snow_mm_h', 'weather_day_id wind_speed_kmh',\n",
    "                    'weather_day_id prec_total_mm_h', 'weather_day_id temp_felt_max_c', 'weather_day_id temp_felt_min_c', 'weather_day_id humidity_mean_pct',\n",
    "                    'weather_day_id wind_speed_max_kmh', 'weather_day_id cloud_cover_max_pct', 'weather_day_id cloud_cover_min_pct', 'weather_day_id cloud_cover_mean_pct',\n",
    "                    'weather_hour_id temp_c', 'weather_hour_id sunshine_h', 'weather_hour_id temp_felt_c', 'weather_hour_id humidity_pct',\n",
    "                    'weather_hour_id prec_rain_mm_h', 'weather_hour_id prec_snow_mm_h', 'weather_day_id wind_direction_sin', \n",
    "                    'weather_day_id wind_direction_cos', 'weather_hour_id wind_speed_kmh', 'weather_hour_id cloud_cover_pct', 'weather_hour_id prec_total_mm_h', \n",
    "                    'weather_hour_id forecast_distance_h', 'weather_hour_id wind_strength', 'weather_day_id wind_strength', 'weather_day_id moon_phase_cos']\n",
    "\n",
    "# preprocessing of the in the different lists defined columns in the step before\n",
    "for column in embedded_features:\n",
    "    all_sessions = label_encode_column(all_sessions, column)\n",
    "    all_sessions = move_column_to_the_front(all_sessions, column)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "for column in ordinal_features:\n",
    "    all_sessions[column] = scaler.fit_transform(np.array(all_sessions[column]).reshape(-1, 1))\n",
    "\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "for column in ohe_features:\n",
    "    all_sessions = onehot_encode_column(all_sessions, column)\n",
    "\n",
    "for column in binary_features:\n",
    "    all_sessions = binary_encode_column(all_sessions, column)\n",
    "\n",
    "# get the amount of unique entries in the classification column\n",
    "unique_classifications = all_sessions['content_id'].nunique()\n",
    "\n",
    "# add (n-1)th and (n-2)th content_id. Move them to front due to the necessary embedding later on and substitute null values.\n",
    "all_sessions[f'prev_content_id'] = all_sessions['content_id'].shift(1)\n",
    "all_sessions[f'prev_prev_content_id'] = all_sessions['content_id'].shift(2)\n",
    "all_sessions = move_column_to_the_front(all_sessions, 'prev_prev_content_id')\n",
    "all_sessions = move_column_to_the_front(all_sessions, 'prev_content_id')\n",
    "all_sessions.fillna({'prev_content_id': unique_classifications + 1}, inplace=True)\n",
    "all_sessions.fillna({'prev_prev_content_id': unique_classifications + 1}, inplace=True)\n",
    "\n",
    "grouped_df = all_sessions.groupby(by=[\"session_id\"])\n",
    "\n",
    "feature_vectors = build_feature_vectors(grouped_df, 'content_id')\n",
    "input_features, classification_labels = split_feature_vectors(feature_vectors)\n",
    "\n",
    "# calculate average sequence length of all sessions\n",
    "all_sequence_lengths = [len(seq) for seq in input_features]\n",
    "sequence_length = int(sum(all_sequence_lengths) / len(all_sequence_lengths))\n",
    "\n",
    "# build padded input_features and classification_labels and transform them to arrays\n",
    "input_features_padded = pad_input_features(input_features, sequence_length)\n",
    "classification_labels_padded = pad_classifications(classification_labels, sequence_length)\n",
    "input_features_padded = np.array(input_features_padded)\n",
    "classification_labels_padded = np.array(classification_labels_padded)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_features_padded, classification_labels_padded, test_size=0.2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the X_train and X_test arrays to get the necessary data for the embedding layers\n",
    "X_train_prev_event, X_train = split_arrays(X_train)\n",
    "X_train_prev_prev_event, X_train = split_arrays(X_train)\n",
    "X_train_session_id, X_train = split_arrays(X_train)\n",
    "X_train_device_id, X_train = split_arrays(X_train)\n",
    "X_train_content_id, X_train = split_arrays(X_train)\n",
    "\n",
    "X_test_prev_prev_event, X_test = split_arrays(X_test)\n",
    "X_test_prev_event, X_test = split_arrays(X_test)\n",
    "X_test_session_id, X_test = split_arrays(X_test)\n",
    "X_test_device_id, X_test = split_arrays(X_test)\n",
    "X_test_content_id, X_test = split_arrays(X_test)\n",
    "\n",
    "# calculate the input dimension for the embedding layer\n",
    "X_train_prev_event_input_dim = get_embedding_input_dim(X_train_prev_event)\n",
    "X_train_prev_prev_event_input_dim = get_embedding_input_dim(X_train_prev_prev_event)\n",
    "X_train_session_id_input_dim = get_embedding_input_dim(X_train_session_id)\n",
    "X_train_device_id_input_dim = get_embedding_input_dim(X_train_device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 02m 11s]\n",
      "val_loss: 4123.864420572917\n",
      "\n",
      "Best val_loss So Far: 11.151456514994303\n",
      "Total elapsed time: 00h 27m 03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabio\\miniconda3\\envs\\BA\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:418: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 76 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.1111 - loss: 11.1456\n",
      "[test loss, test accuracy]: [11.145646095275879, 0.1111111119389534]\n",
      "Reloading Tuner from trained_models\\05.06_1_XX%_Smaller\\tuner0.json\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.3333 - loss: 10.6050\n",
      "[test loss, test accuracy]: [10.60498332977295, 0.3333333432674408]\n",
      "Reloading Tuner from trained_models\\05.06_1_XX%_Smaller\\tuner0.json\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.2222 - loss: 10.8899\n",
      "[test loss, test accuracy]: [10.889859199523926, 0.2222222238779068]\n",
      "Reloading Tuner from trained_models\\05.06_1_XX%_Smaller\\tuner0.json\n",
      "WARNING:tensorflow:5 out of the last 54 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x000001579BEF6F20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.1111 - loss: 10.9840\n",
      "[test loss, test accuracy]: [10.984013557434082, 0.1111111119389534]\n",
      "Reloading Tuner from trained_models\\05.06_1_XX%_Smaller\\tuner0.json\n",
      "WARNING:tensorflow:6 out of the last 55 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x00000157DE84F4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.3333 - loss: 10.6821\n",
      "[test loss, test accuracy]: [10.682125091552734, 0.3333333432674408]\n",
      "Average validation accuracy across all folds: 0.22222222685813903\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabio\\miniconda3\\envs\\BA\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:418: UserWarning: Skipping variable loading for optimizer 'adam', because it has 76 variables whereas the saved optimizer has 2 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 29s/step - accuracy: 0.2424 - loss: 10.8026 - val_accuracy: 0.1111 - val_loss: 162.9265\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.2424 - loss: 162.4496 - val_accuracy: 0.2222 - val_loss: 17.3947\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.3636 - loss: 16.8629 - val_accuracy: 0.2222 - val_loss: 36.7780\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - accuracy: 0.2727 - loss: 36.2411 - val_accuracy: 0.2222 - val_loss: 91.5471\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.2727 - loss: 91.0116 - val_accuracy: 0.2222 - val_loss: 63.4239\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 918ms/step - accuracy: 0.2727 - loss: 62.8549 - val_accuracy: 0.2222 - val_loss: 14.9373\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 779ms/step - accuracy: 0.2727 - loss: 14.3152 - val_accuracy: 0.2222 - val_loss: 6.4529\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 853ms/step - accuracy: 0.2727 - loss: 5.7726 - val_accuracy: 0.2222 - val_loss: 32.9111\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 986ms/step - accuracy: 0.2727 - loss: 32.1866 - val_accuracy: 0.2222 - val_loss: 48.3527\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 798ms/step - accuracy: 0.2727 - loss: 47.6101 - val_accuracy: 0.2222 - val_loss: 34.9157\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 742ms/step - accuracy: 0.2727 - loss: 34.1613 - val_accuracy: 0.2222 - val_loss: 12.9081\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.2727 - loss: 12.1395 - val_accuracy: 0.2222 - val_loss: 5.4663\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 0.2727 - loss: 4.6990 - val_accuracy: 0.2222 - val_loss: 14.5025\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.2727 - loss: 13.7480 - val_accuracy: 0.2222 - val_loss: 24.5230\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.2727 - loss: 23.7719 - val_accuracy: 0.2222 - val_loss: 23.5359\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - accuracy: 0.2727 - loss: 22.7616 - val_accuracy: 0.2222 - val_loss: 14.3526\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.2727 - loss: 13.5387 - val_accuracy: 0.2222 - val_loss: 7.1010\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - accuracy: 0.2727 - loss: 6.2488 - val_accuracy: 0.2222 - val_loss: 7.1240\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - accuracy: 0.2727 - loss: 6.2374 - val_accuracy: 0.2222 - val_loss: 11.4907\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.2727 - loss: 10.5753 - val_accuracy: 0.2222 - val_loss: 14.1528\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.2727 - loss: 13.2230 - val_accuracy: 0.2222 - val_loss: 12.4087\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.2727 - loss: 11.4741 - val_accuracy: 0.2222 - val_loss: 8.4015\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - accuracy: 0.2727 - loss: 7.4664 - val_accuracy: 0.2222 - val_loss: 5.7533\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.2727 - loss: 4.8218 - val_accuracy: 0.2222 - val_loss: 6.0474\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.2727 - loss: 5.1239 - val_accuracy: 0.2222 - val_loss: 7.9449\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.2727 - loss: 7.0225 - val_accuracy: 0.2222 - val_loss: 8.8922\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.2727 - loss: 7.9609 - val_accuracy: 0.2222 - val_loss: 7.7026\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.2727 - loss: 6.7559 - val_accuracy: 0.2222 - val_loss: 5.5585\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.2727 - loss: 4.5870 - val_accuracy: 0.2222 - val_loss: 4.4796\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 733ms/step - accuracy: 0.2727 - loss: 3.4883 - val_accuracy: 0.2222 - val_loss: 5.1240\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790ms/step - accuracy: 0.2727 - loss: 4.1324 - val_accuracy: 0.2222 - val_loss: 6.2386\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 810ms/step - accuracy: 0.3636 - loss: 5.2393 - val_accuracy: 0.2222 - val_loss: 6.2449\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668ms/step - accuracy: 0.3636 - loss: 5.2303 - val_accuracy: 0.2222 - val_loss: 5.0844\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 634ms/step - accuracy: 0.3636 - loss: 4.0586 - val_accuracy: 0.2222 - val_loss: 4.0483\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 822ms/step - accuracy: 0.3636 - loss: 3.0194 - val_accuracy: 0.2222 - val_loss: 4.0758\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 566ms/step - accuracy: 0.3636 - loss: 3.0395 - val_accuracy: 0.2222 - val_loss: 4.7231\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309ms/step - accuracy: 0.3636 - loss: 3.6770 - val_accuracy: 0.2222 - val_loss: 4.9569\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - accuracy: 0.3636 - loss: 3.9016 - val_accuracy: 0.2222 - val_loss: 4.4724\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.3636 - loss: 3.4060 - val_accuracy: 0.2222 - val_loss: 3.8496\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.3636 - loss: 2.7722 - val_accuracy: 0.2222 - val_loss: 3.6806\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.3636 - loss: 2.6012 - val_accuracy: 0.2222 - val_loss: 3.9644\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - accuracy: 0.3636 - loss: 2.8578 - val_accuracy: 0.2222 - val_loss: 4.1560\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.3636 - loss: 3.0561 - val_accuracy: 0.2222 - val_loss: 4.0281\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.3636 - loss: 2.8951 - val_accuracy: 0.2222 - val_loss: 3.6522\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.3636 - loss: 2.5391 - val_accuracy: 0.2222 - val_loss: 3.5117\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - accuracy: 0.3636 - loss: 2.3549 - val_accuracy: 0.2222 - val_loss: 3.5650\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.3636 - loss: 2.4521 - val_accuracy: 0.2222 - val_loss: 3.7432\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.3636 - loss: 2.5910 - val_accuracy: 0.2222 - val_loss: 3.6937\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.3636 - loss: 2.5406 - val_accuracy: 0.2222 - val_loss: 3.4794\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.3636 - loss: 2.3232 - val_accuracy: 0.2222 - val_loss: 3.3647\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4167 - loss: 2.3359\n",
      "Test loss: 2.335947275161743, Test accuracy: 0.4166666865348816\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "Predicted classes:\n",
      "y_test 0: 6 0 0\n",
      "y_test 1: 6 0 0\n",
      "y_test 2: 6 0 0\n",
      "y_test 3: 6 0 0\n",
      "Epoch 1/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 27s/step - accuracy: 0.2727 - loss: 10.8021 - val_accuracy: 0.1111 - val_loss: 162.9159\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 702ms/step - accuracy: 0.2424 - loss: 162.4468 - val_accuracy: 0.2222 - val_loss: 17.4035\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 739ms/step - accuracy: 0.2727 - loss: 16.8610 - val_accuracy: 0.2222 - val_loss: 36.7719\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 834ms/step - accuracy: 0.2727 - loss: 36.2408 - val_accuracy: 0.2222 - val_loss: 91.5394\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 504ms/step - accuracy: 0.2727 - loss: 91.0064 - val_accuracy: 0.2222 - val_loss: 63.4279\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.2727 - loss: 62.8529 - val_accuracy: 0.2222 - val_loss: 14.9292\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.2727 - loss: 14.3148 - val_accuracy: 0.2222 - val_loss: 6.4376\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.2727 - loss: 5.7707 - val_accuracy: 0.2222 - val_loss: 32.9069\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.2727 - loss: 32.1844 - val_accuracy: 0.2222 - val_loss: 48.3560\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.2727 - loss: 47.6085 - val_accuracy: 0.2222 - val_loss: 34.9122\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 0.2727 - loss: 34.1592 - val_accuracy: 0.2222 - val_loss: 12.8979\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - accuracy: 0.2727 - loss: 12.1382 - val_accuracy: 0.2222 - val_loss: 5.4665\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - accuracy: 0.2727 - loss: 4.6987 - val_accuracy: 0.2222 - val_loss: 14.5087\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - accuracy: 0.2727 - loss: 13.7483 - val_accuracy: 0.2222 - val_loss: 24.5197\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.2727 - loss: 23.7716 - val_accuracy: 0.2222 - val_loss: 23.5223\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.2727 - loss: 22.7592 - val_accuracy: 0.2222 - val_loss: 14.3486\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.2727 - loss: 13.5377 - val_accuracy: 0.2222 - val_loss: 7.1095\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.2727 - loss: 6.2489 - val_accuracy: 0.2222 - val_loss: 7.1307\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.2727 - loss: 6.2355 - val_accuracy: 0.2222 - val_loss: 11.4938\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.2727 - loss: 10.5755 - val_accuracy: 0.2222 - val_loss: 14.1574\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.2727 - loss: 13.2239 - val_accuracy: 0.2222 - val_loss: 12.4127\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.2727 - loss: 11.4757 - val_accuracy: 0.2222 - val_loss: 8.4032\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.2727 - loss: 7.4668 - val_accuracy: 0.2222 - val_loss: 5.7565\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 599ms/step - accuracy: 0.2727 - loss: 4.8236 - val_accuracy: 0.2222 - val_loss: 6.0511\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 689ms/step - accuracy: 0.2727 - loss: 5.1249 - val_accuracy: 0.2222 - val_loss: 7.9412\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 779ms/step - accuracy: 0.2727 - loss: 7.0228 - val_accuracy: 0.2222 - val_loss: 8.8810\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 883ms/step - accuracy: 0.2727 - loss: 7.9588 - val_accuracy: 0.2222 - val_loss: 7.6985\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 717ms/step - accuracy: 0.2727 - loss: 6.7578 - val_accuracy: 0.2222 - val_loss: 5.5528\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668ms/step - accuracy: 0.2727 - loss: 4.5884 - val_accuracy: 0.2222 - val_loss: 4.4713\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 698ms/step - accuracy: 0.2727 - loss: 3.4880 - val_accuracy: 0.2222 - val_loss: 5.1237\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - accuracy: 0.2727 - loss: 4.1314 - val_accuracy: 0.2222 - val_loss: 6.2443\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 0.3636 - loss: 5.2402 - val_accuracy: 0.2222 - val_loss: 6.2452\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 0.3636 - loss: 5.2315 - val_accuracy: 0.2222 - val_loss: 5.0848\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - accuracy: 0.3636 - loss: 4.0590 - val_accuracy: 0.2222 - val_loss: 4.0547\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.3636 - loss: 3.0197 - val_accuracy: 0.2222 - val_loss: 4.0755\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.3636 - loss: 3.0398 - val_accuracy: 0.2222 - val_loss: 4.7180\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.3636 - loss: 3.6759 - val_accuracy: 0.2222 - val_loss: 4.9527\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.3636 - loss: 3.9029 - val_accuracy: 0.2222 - val_loss: 4.4675\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.3636 - loss: 3.4071 - val_accuracy: 0.2222 - val_loss: 3.8321\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.3636 - loss: 2.7702 - val_accuracy: 0.2222 - val_loss: 3.7044\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step - accuracy: 0.3636 - loss: 2.6024 - val_accuracy: 0.2222 - val_loss: 3.9450\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - accuracy: 0.3636 - loss: 2.8577 - val_accuracy: 0.2222 - val_loss: 4.1601\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.3636 - loss: 3.0574 - val_accuracy: 0.2222 - val_loss: 3.9961\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.3636 - loss: 2.8919 - val_accuracy: 0.2222 - val_loss: 3.6628\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.3636 - loss: 2.5352 - val_accuracy: 0.2222 - val_loss: 3.4841\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step - accuracy: 0.3636 - loss: 2.3532 - val_accuracy: 0.2222 - val_loss: 3.5899\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.3636 - loss: 2.4467 - val_accuracy: 0.2222 - val_loss: 3.7502\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - accuracy: 0.3636 - loss: 2.5943 - val_accuracy: 0.2222 - val_loss: 3.6759\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314ms/step - accuracy: 0.3636 - loss: 2.5400 - val_accuracy: 0.2222 - val_loss: 3.4831\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 629ms/step - accuracy: 0.3636 - loss: 2.3205 - val_accuracy: 0.2222 - val_loss: 3.3398\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.4167 - loss: 2.3313\n",
      "Test loss: 2.3313448429107666, Test accuracy: 0.4166666865348816\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "Predicted classes:\n",
      "y_test 0: 6 0 0\n",
      "y_test 1: 6 0 0\n",
      "y_test 2: 6 0 0\n",
      "y_test 3: 6 0 0\n",
      "Epoch 1/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 22s/step - accuracy: 0.2424 - loss: 10.8024 - val_accuracy: 0.1111 - val_loss: 162.9265\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.2424 - loss: 162.4492 - val_accuracy: 0.2222 - val_loss: 17.3818\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.2727 - loss: 16.8604 - val_accuracy: 0.2222 - val_loss: 36.7791\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.2727 - loss: 36.2427 - val_accuracy: 0.2222 - val_loss: 91.5446\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.2727 - loss: 91.0071 - val_accuracy: 0.2222 - val_loss: 63.4206\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.2727 - loss: 62.8523 - val_accuracy: 0.2222 - val_loss: 14.9392\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.2727 - loss: 14.3154 - val_accuracy: 0.2222 - val_loss: 6.4456\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.2727 - loss: 5.7710 - val_accuracy: 0.2222 - val_loss: 32.9067\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.2727 - loss: 32.1870 - val_accuracy: 0.2222 - val_loss: 48.3515\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.2727 - loss: 47.6084 - val_accuracy: 0.2222 - val_loss: 34.9157\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.2727 - loss: 34.1588 - val_accuracy: 0.2222 - val_loss: 12.9055\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.2727 - loss: 12.1389 - val_accuracy: 0.2222 - val_loss: 5.4660\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.2727 - loss: 4.6988 - val_accuracy: 0.2222 - val_loss: 14.5074\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.2727 - loss: 13.7496 - val_accuracy: 0.2222 - val_loss: 24.5315\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - accuracy: 0.2727 - loss: 23.7722 - val_accuracy: 0.2222 - val_loss: 23.5410\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.2727 - loss: 22.7602 - val_accuracy: 0.2222 - val_loss: 14.3494\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.2727 - loss: 13.5362 - val_accuracy: 0.2222 - val_loss: 7.1026\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.2727 - loss: 6.2475 - val_accuracy: 0.2222 - val_loss: 7.1347\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.2727 - loss: 6.2364 - val_accuracy: 0.2222 - val_loss: 11.4991\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.2727 - loss: 10.5761 - val_accuracy: 0.2222 - val_loss: 14.1499\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.2727 - loss: 13.2229 - val_accuracy: 0.2222 - val_loss: 12.3979\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.2727 - loss: 11.4751 - val_accuracy: 0.2222 - val_loss: 8.3872\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.2727 - loss: 7.4661 - val_accuracy: 0.2222 - val_loss: 5.7473\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.2727 - loss: 4.8199 - val_accuracy: 0.2222 - val_loss: 6.0514\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.2727 - loss: 5.1260 - val_accuracy: 0.2222 - val_loss: 7.9425\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.2727 - loss: 7.0233 - val_accuracy: 0.2222 - val_loss: 8.8931\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.2727 - loss: 7.9602 - val_accuracy: 0.2222 - val_loss: 7.7184\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.2727 - loss: 6.7586 - val_accuracy: 0.2222 - val_loss: 5.5609\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299ms/step - accuracy: 0.2727 - loss: 4.5883 - val_accuracy: 0.2222 - val_loss: 4.4783\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.2727 - loss: 3.4888 - val_accuracy: 0.2222 - val_loss: 5.1425\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.3636 - loss: 4.1314 - val_accuracy: 0.2222 - val_loss: 6.2538\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.3636 - loss: 5.2393 - val_accuracy: 0.2222 - val_loss: 6.2429\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.3636 - loss: 5.2325 - val_accuracy: 0.2222 - val_loss: 5.0871\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.3636 - loss: 4.0586 - val_accuracy: 0.2222 - val_loss: 4.0588\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.3636 - loss: 3.0205 - val_accuracy: 0.2222 - val_loss: 4.0733\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.3636 - loss: 3.0398 - val_accuracy: 0.2222 - val_loss: 4.7289\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.3636 - loss: 3.6764 - val_accuracy: 0.2222 - val_loss: 4.9570\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.3636 - loss: 3.9042 - val_accuracy: 0.2222 - val_loss: 4.4561\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 401ms/step - accuracy: 0.3636 - loss: 3.4072 - val_accuracy: 0.2222 - val_loss: 3.8381\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.3636 - loss: 2.7709 - val_accuracy: 0.2222 - val_loss: 3.6878\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.3636 - loss: 2.6025 - val_accuracy: 0.2222 - val_loss: 3.9394\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447ms/step - accuracy: 0.3636 - loss: 2.8606 - val_accuracy: 0.2222 - val_loss: 4.1641\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.3636 - loss: 3.0583 - val_accuracy: 0.2222 - val_loss: 3.9864\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.3636 - loss: 2.8943 - val_accuracy: 0.2222 - val_loss: 3.6523\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.3636 - loss: 2.5372 - val_accuracy: 0.2222 - val_loss: 3.4812\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.3636 - loss: 2.3501 - val_accuracy: 0.2222 - val_loss: 3.5786\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.3636 - loss: 2.4478 - val_accuracy: 0.2222 - val_loss: 3.7482\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.3636 - loss: 2.5918 - val_accuracy: 0.2222 - val_loss: 3.7152\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.3636 - loss: 2.5426 - val_accuracy: 0.2222 - val_loss: 3.4910\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.3636 - loss: 2.3208 - val_accuracy: 0.2222 - val_loss: 3.4141\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4167 - loss: 2.3445\n",
      "Test loss: 2.3445165157318115, Test accuracy: 0.4166666865348816\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Predicted classes:\n",
      "y_test 0: 6 0 0\n",
      "y_test 1: 6 0 0\n",
      "y_test 2: 6 0 0\n",
      "y_test 3: 6 0 0\n",
      "Epoch 1/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17s/step - accuracy: 0.2424 - loss: 10.8026 - val_accuracy: 0.1111 - val_loss: 162.9266\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301ms/step - accuracy: 0.2424 - loss: 162.4436 - val_accuracy: 0.2222 - val_loss: 17.3747\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.2727 - loss: 16.8602 - val_accuracy: 0.2222 - val_loss: 36.7859\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.2727 - loss: 36.2435 - val_accuracy: 0.2222 - val_loss: 91.5479\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.2727 - loss: 91.0103 - val_accuracy: 0.2222 - val_loss: 63.4186\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.2727 - loss: 62.8504 - val_accuracy: 0.2222 - val_loss: 14.9376\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.2727 - loss: 14.3115 - val_accuracy: 0.2222 - val_loss: 6.4522\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.2727 - loss: 5.7733 - val_accuracy: 0.2222 - val_loss: 32.9009\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.2727 - loss: 32.1895 - val_accuracy: 0.2222 - val_loss: 48.3435\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.2727 - loss: 47.6121 - val_accuracy: 0.2222 - val_loss: 34.9121\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.2727 - loss: 34.1567 - val_accuracy: 0.2222 - val_loss: 12.9074\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.2727 - loss: 12.1361 - val_accuracy: 0.2222 - val_loss: 5.4670\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.2727 - loss: 4.6982 - val_accuracy: 0.2222 - val_loss: 14.5055\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.2727 - loss: 13.7495 - val_accuracy: 0.2222 - val_loss: 24.5357\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.2727 - loss: 23.7728 - val_accuracy: 0.2222 - val_loss: 23.5492\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.2727 - loss: 22.7593 - val_accuracy: 0.2222 - val_loss: 14.3599\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.2727 - loss: 13.5363 - val_accuracy: 0.2222 - val_loss: 7.1003\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.2727 - loss: 6.2474 - val_accuracy: 0.2222 - val_loss: 7.1276\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.2727 - loss: 6.2370 - val_accuracy: 0.2222 - val_loss: 11.4999\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.2727 - loss: 10.5764 - val_accuracy: 0.2222 - val_loss: 14.1549\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.2727 - loss: 13.2230 - val_accuracy: 0.2222 - val_loss: 12.3987\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.2727 - loss: 11.4760 - val_accuracy: 0.2222 - val_loss: 8.3886\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.2727 - loss: 7.4671 - val_accuracy: 0.2222 - val_loss: 5.7490\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.2727 - loss: 4.8213 - val_accuracy: 0.2222 - val_loss: 6.0503\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.2727 - loss: 5.1246 - val_accuracy: 0.2222 - val_loss: 7.9429\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.2727 - loss: 7.0231 - val_accuracy: 0.2222 - val_loss: 8.8879\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.2727 - loss: 7.9600 - val_accuracy: 0.2222 - val_loss: 7.7104\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.2727 - loss: 6.7589 - val_accuracy: 0.2222 - val_loss: 5.5638\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.2727 - loss: 4.5884 - val_accuracy: 0.2222 - val_loss: 4.4724\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.2727 - loss: 3.4885 - val_accuracy: 0.2222 - val_loss: 5.1289\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.3636 - loss: 4.1322 - val_accuracy: 0.2222 - val_loss: 6.2564\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.3636 - loss: 5.2393 - val_accuracy: 0.2222 - val_loss: 6.2650\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.3636 - loss: 5.2327 - val_accuracy: 0.2222 - val_loss: 5.0786\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.3636 - loss: 4.0591 - val_accuracy: 0.2222 - val_loss: 4.0461\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.3636 - loss: 3.0209 - val_accuracy: 0.2222 - val_loss: 4.0879\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - accuracy: 0.3636 - loss: 3.0380 - val_accuracy: 0.2222 - val_loss: 4.7169\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.3636 - loss: 3.6768 - val_accuracy: 0.2222 - val_loss: 4.9495\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.3636 - loss: 3.9036 - val_accuracy: 0.2222 - val_loss: 4.4841\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.3636 - loss: 3.4075 - val_accuracy: 0.2222 - val_loss: 3.8390\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.3636 - loss: 2.7714 - val_accuracy: 0.2222 - val_loss: 3.6708\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.3636 - loss: 2.6022 - val_accuracy: 0.2222 - val_loss: 3.9552\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.3636 - loss: 2.8602 - val_accuracy: 0.2222 - val_loss: 4.1362\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.3636 - loss: 3.0560 - val_accuracy: 0.2222 - val_loss: 4.0249\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step - accuracy: 0.3636 - loss: 2.8953 - val_accuracy: 0.2222 - val_loss: 3.6521\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.3636 - loss: 2.5355 - val_accuracy: 0.2222 - val_loss: 3.4984\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.3636 - loss: 2.3496 - val_accuracy: 0.2222 - val_loss: 3.5879\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262ms/step - accuracy: 0.3636 - loss: 2.4463 - val_accuracy: 0.2222 - val_loss: 3.7460\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.3636 - loss: 2.5932 - val_accuracy: 0.2222 - val_loss: 3.7111\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.3636 - loss: 2.5379 - val_accuracy: 0.2222 - val_loss: 3.4885\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.3636 - loss: 2.3200 - val_accuracy: 0.2222 - val_loss: 3.3961\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4167 - loss: 2.3371\n",
      "Test loss: 2.3370726108551025, Test accuracy: 0.4166666865348816\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Predicted classes:\n",
      "y_test 0: 6 0 0\n",
      "y_test 1: 6 0 0\n",
      "y_test 2: 6 0 0\n",
      "y_test 3: 6 0 0\n",
      "Epoch 1/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18s/step - accuracy: 0.2424 - loss: 10.8027 - val_accuracy: 0.1111 - val_loss: 162.9313\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.2424 - loss: 162.4501 - val_accuracy: 0.2222 - val_loss: 17.3681\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step - accuracy: 0.2727 - loss: 16.8625 - val_accuracy: 0.2222 - val_loss: 36.7947\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step - accuracy: 0.2727 - loss: 36.2430 - val_accuracy: 0.2222 - val_loss: 91.5468\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step - accuracy: 0.2727 - loss: 91.0101 - val_accuracy: 0.2222 - val_loss: 63.4102\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.2727 - loss: 62.8547 - val_accuracy: 0.2222 - val_loss: 14.9374\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 893ms/step - accuracy: 0.2727 - loss: 14.3148 - val_accuracy: 0.2222 - val_loss: 6.4641\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step - accuracy: 0.2727 - loss: 5.7740 - val_accuracy: 0.2222 - val_loss: 32.9097\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 873ms/step - accuracy: 0.2727 - loss: 32.1866 - val_accuracy: 0.2222 - val_loss: 48.3410\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 874ms/step - accuracy: 0.2727 - loss: 47.6081 - val_accuracy: 0.2222 - val_loss: 34.9049\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 341ms/step - accuracy: 0.2727 - loss: 34.1608 - val_accuracy: 0.2222 - val_loss: 12.9045\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - accuracy: 0.2727 - loss: 12.1397 - val_accuracy: 0.2222 - val_loss: 5.4725\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.2727 - loss: 4.6997 - val_accuracy: 0.2222 - val_loss: 14.5077\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.2727 - loss: 13.7500 - val_accuracy: 0.2222 - val_loss: 24.5215\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step - accuracy: 0.2727 - loss: 23.7723 - val_accuracy: 0.2222 - val_loss: 23.5267\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.2727 - loss: 22.7617 - val_accuracy: 0.2222 - val_loss: 14.3459\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.2727 - loss: 13.5399 - val_accuracy: 0.2222 - val_loss: 7.1000\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - accuracy: 0.2727 - loss: 6.2503 - val_accuracy: 0.2222 - val_loss: 7.1250\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.2727 - loss: 6.2378 - val_accuracy: 0.2222 - val_loss: 11.4928\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 0.2727 - loss: 10.5757 - val_accuracy: 0.2222 - val_loss: 14.1529\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.2727 - loss: 13.2244 - val_accuracy: 0.2222 - val_loss: 12.4038\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.2727 - loss: 11.4746 - val_accuracy: 0.2222 - val_loss: 8.3961\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step - accuracy: 0.2727 - loss: 7.4687 - val_accuracy: 0.2222 - val_loss: 5.7520\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.2727 - loss: 4.8235 - val_accuracy: 0.2222 - val_loss: 6.0510\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.2727 - loss: 5.1262 - val_accuracy: 0.2222 - val_loss: 7.9414\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.2727 - loss: 7.0249 - val_accuracy: 0.2222 - val_loss: 8.8822\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.2727 - loss: 7.9592 - val_accuracy: 0.2222 - val_loss: 7.7001\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.2727 - loss: 6.7587 - val_accuracy: 0.2222 - val_loss: 5.5582\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.2727 - loss: 4.5886 - val_accuracy: 0.2222 - val_loss: 4.4769\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 595ms/step - accuracy: 0.2727 - loss: 3.4887 - val_accuracy: 0.2222 - val_loss: 5.1301\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.2727 - loss: 4.1334 - val_accuracy: 0.2222 - val_loss: 6.2465\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3636 - loss: 5.2391 - val_accuracy: 0.2222 - val_loss: 6.2506\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.3636 - loss: 5.2316 - val_accuracy: 0.2222 - val_loss: 5.0798\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 814ms/step - accuracy: 0.3636 - loss: 4.0589 - val_accuracy: 0.2222 - val_loss: 4.0485\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 275ms/step - accuracy: 0.3636 - loss: 3.0221 - val_accuracy: 0.2222 - val_loss: 4.0704\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.3636 - loss: 3.0418 - val_accuracy: 0.2222 - val_loss: 4.7037\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step - accuracy: 0.3636 - loss: 3.6763 - val_accuracy: 0.2222 - val_loss: 4.9498\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - accuracy: 0.3636 - loss: 3.9053 - val_accuracy: 0.2222 - val_loss: 4.4575\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316ms/step - accuracy: 0.3636 - loss: 3.4061 - val_accuracy: 0.2222 - val_loss: 3.8283\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step - accuracy: 0.3636 - loss: 2.7707 - val_accuracy: 0.2222 - val_loss: 3.6855\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - accuracy: 0.3636 - loss: 2.6032 - val_accuracy: 0.2222 - val_loss: 3.9521\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.3636 - loss: 2.8583 - val_accuracy: 0.2222 - val_loss: 4.1531\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 274ms/step - accuracy: 0.3636 - loss: 3.0556 - val_accuracy: 0.2222 - val_loss: 4.0133\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 259ms/step - accuracy: 0.3636 - loss: 2.8927 - val_accuracy: 0.2222 - val_loss: 3.6505\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.3636 - loss: 2.5400 - val_accuracy: 0.2222 - val_loss: 3.4901\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.3636 - loss: 2.3499 - val_accuracy: 0.2222 - val_loss: 3.5882\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step - accuracy: 0.3636 - loss: 2.4425 - val_accuracy: 0.2222 - val_loss: 3.7682\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.3636 - loss: 2.5973 - val_accuracy: 0.2222 - val_loss: 3.6886\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 273ms/step - accuracy: 0.3636 - loss: 2.5423 - val_accuracy: 0.2222 - val_loss: 3.5235\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step - accuracy: 0.3636 - loss: 2.3244 - val_accuracy: 0.2222 - val_loss: 3.3415\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4167 - loss: 2.3326\n",
      "Test loss: 2.332575798034668, Test accuracy: 0.4166666865348816\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000157907A1F80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "Predicted classes:\n",
      "y_test 0: 6 0 0\n",
      "y_test 1: 6 0 0\n",
      "y_test 2: 6 0 0\n",
      "y_test 3: 6 0 0\n",
      "[(2.335947275161743, 0.4166666865348816), (2.3313448429107666, 0.4166666865348816), (2.3445165157318115, 0.4166666865348816), (2.3370726108551025, 0.4166666865348816), (2.332575798034668, 0.4166666865348816)]\n"
     ]
    }
   ],
   "source": [
    "def model_builder(hp):\n",
    "    # hyperparameters to tune for the output dimension of the embedding layers\n",
    "    hp_output1 = hp.Int('output_dim_prev_event', min_value=5, max_value=40, step=5)\n",
    "    hp_output2 = hp.Int('output_dim_prev_prev_event', min_value=5, max_value=40, step=5)\n",
    "    hp_output3 = hp.Int('output_dim_session_id', min_value=5, max_value=40, step=5)\n",
    "    hp_output4 = hp.Int('output_dim_device_id', min_value=5, max_value=40, step=5)\n",
    "    hp_output5 = hp.Int('output_dim_output', min_value=5, max_value=40, step=5)\n",
    "\n",
    "    # embedding layers for the features to embed\n",
    "    embedding_input_prev_event = tf.keras.layers.Input(shape=(X_train_prev_event.shape[1],))\n",
    "    embedding_layer_prev_event = tf.keras.layers.Embedding(input_dim=X_train_prev_event_input_dim, output_dim=hp_output1)(embedding_input_prev_event)\n",
    "\n",
    "    embedding_input_prev_prev_event = tf.keras.layers.Input(shape=(X_train_prev_prev_event.shape[1],))\n",
    "    embedding_layer_prev_prev_event = tf.keras.layers.Embedding(input_dim=X_train_prev_prev_event_input_dim, output_dim=hp_output2)(embedding_input_prev_prev_event)\n",
    "\n",
    "    embedding_input_session_id = tf.keras.layers.Input(shape=(X_train_session_id.shape[1],))\n",
    "    embedding_layer_session_id = tf.keras.layers.Embedding(input_dim=X_train_session_id_input_dim, output_dim=hp_output3)(embedding_input_session_id)\n",
    "\n",
    "    embedding_input_device_id = tf.keras.layers.Input(shape=(X_train_device_id.shape[1],))\n",
    "    embedding_layer_device_id = tf.keras.layers.Embedding(input_dim=X_train_device_id_input_dim, output_dim=hp_output4)(embedding_input_device_id)\n",
    "\n",
    "    embedding_input_content_id = tf.keras.layers.Input(shape=(X_train_content_id.shape[1],))\n",
    "    embedding_layer_content_id = tf.keras.layers.Embedding(input_dim=unique_classifications + 1, output_dim=hp_output5)(embedding_input_content_id)\n",
    "\n",
    "    # input layer for additional non-embedded features\n",
    "    input_features = tf.keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "    # concatenate the embedding output and the additional features\n",
    "    concatenated_embeddings = tf.keras.layers.Concatenate(axis=-1)([embedding_layer_prev_event, embedding_layer_prev_prev_event, embedding_layer_session_id, embedding_layer_device_id, embedding_layer_content_id, input_features])\n",
    "\n",
    "    # hyperparameters to tune for the dense hidden layers \n",
    "    hp_units = hp.Int('units', min_value=16, max_value=1024, step=32)\n",
    "    hp_layers = hp.Int('layers', min_value=2, max_value=15, step=1)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[0.1, 0.01])\n",
    "    hp_optimizer = hp.Choice('optimizer', values=['adam'])\n",
    "    hp_regularization = hp.Choice('regularization', values=['l1', 'l2', 'l1_l2'])\n",
    "    hp_lambda = hp.Float('lambda', min_value=0.5, max_value=1.0, step=0.01)\n",
    "\n",
    "    # add the tuned number of hidden layer groups\n",
    "    for _ in range(hp_layers):\n",
    "        if hp_regularization == 'l1':\n",
    "            regularizer = tf.keras.regularizers.l1(hp_lambda)\n",
    "        elif hp_regularization == 'l2':\n",
    "            regularizer = tf.keras.regularizers.l2(hp_lambda)\n",
    "        else: # l1_l2\n",
    "            regularizer = tf.keras.regularizers.l1_l2(l1=hp_lambda, l2=hp_lambda)\n",
    "\n",
    "        concatenated_embeddings = tf.keras.layers.SimpleRNN(\n",
    "            units=hp_units, \n",
    "            activation='relu', \n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=regularizer,\n",
    "            recurrent_regularizer=regularizer,\n",
    "            bias_regularizer=regularizer,\n",
    "            dropout=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1),\n",
    "            recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.0, max_value=0.5, step=0.1))(concatenated_embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(unique_classifications + 1, activation='softmax')(concatenated_embeddings)\n",
    "    \n",
    "    # create the model\n",
    "    model = tf.keras.models.Model(inputs=[embedding_input_prev_event, embedding_input_prev_prev_event, embedding_input_session_id, embedding_input_device_id, embedding_input_content_id, input_features], outputs=outputs)\n",
    "    \n",
    "    # define optimizer based on hyperparameter result    \n",
    "    optimizer = hp_optimizer\n",
    "    if optimizer == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=hp_learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=hp_learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "best_performance = 0\n",
    "project_name = '05.06_1_XX%_Smaller'\n",
    "best_model_path = r'trained_models\\best_model\\best_model.keras'\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "performance_metrics = []\n",
    "\n",
    "for train_index, val_index in kfold.split(X_train):\n",
    "    # split the train sets into different train_folds and val_folds for every corss validation run\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    X_train_prev_event_fold, X_val_prev_event_fold = X_train_prev_event[train_index], X_train_prev_event[val_index]\n",
    "    X_train_prev_prev_event_fold, X_val_prev_prev_event_fold = X_train_prev_prev_event[train_index], X_train_prev_prev_event[val_index]\n",
    "    X_train_session_fold, X_val_session_fold = X_train_session_id[train_index], X_train_session_id[val_index]\n",
    "    X_train_device_fold, X_val_device_fold = X_train_device_id[train_index], X_train_device_id[val_index]\n",
    "    X_train_output_fold, X_val_output_fold = X_train_content_id[train_index], X_train_content_id[val_index]\n",
    "    \n",
    "    X_train_fold_inputs = [X_train_prev_event_fold, X_train_prev_prev_event_fold, X_train_session_fold, X_train_device_fold, X_train_output_fold, X_train_fold]\n",
    "    X_val_fold_inputs = [X_val_prev_event_fold, X_val_prev_prev_event_fold, X_val_session_fold, X_val_device_fold, X_val_output_fold, X_val_fold]\n",
    "\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    tuner = kt.BayesianOptimization(\n",
    "        model_builder,\n",
    "        objective='val_loss',\n",
    "        max_trials=10,\n",
    "        executions_per_trial=3,\n",
    "        directory='trained_models',\n",
    "        project_name=project_name\n",
    "    )\n",
    "\n",
    "    # find the best hyperparamters\n",
    "    tuner.search(X_train_fold_inputs, y_train_fold, epochs=50, validation_data=(X_val_fold_inputs, y_val_fold))\n",
    "    \n",
    "    # get bets model from the executions per trial tested ones\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    # evaluate the model with the current hyperparameters, print them and save them in an extra list\n",
    "    eval_result = best_model.evaluate(X_val_fold_inputs, y_val_fold)\n",
    "    print(f\"[test loss, test accuracy]: {eval_result}\")\n",
    "    performance_metrics.append(eval_result[1])\n",
    "\n",
    "    # save the model if it is better than the current best\n",
    "    if eval_result[1] > best_performance:\n",
    "        best_performance = eval_result[1]\n",
    "        best_model.save(best_model_path)\n",
    "\n",
    "X_train_inputs = [X_train_prev_event, X_train_prev_prev_event, X_train_session_id, X_train_device_id, X_train_content_id, X_train]\n",
    "X_test_inputs = [X_test_prev_event, X_test_prev_prev_event, X_test_session_id, X_test_device_id, X_test_content_id, X_test]\n",
    "\n",
    "# average the performance over the cross validation process\n",
    "average_performance = np.mean(performance_metrics)\n",
    "print(f\"Average validation accuracy across all folds: {average_performance}\")\n",
    "\n",
    "# train the model 5 times, print the results and save them in an extra list\n",
    "history_test = []\n",
    "for i in range(5):\n",
    "    # load the best model after optimizing for hyperparameters\n",
    "    best_model = keras.models.load_model(best_model_path, safe_mode=False)\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    \n",
    "    history = best_model.fit(X_train_inputs, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    test_loss, test_accuracy = best_model.evaluate(X_test_inputs, y_test)\n",
    "    history_test.append((test_loss, test_accuracy))\n",
    "    print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")\n",
    "\n",
    "    predictions = best_model.predict(X_test_inputs)\n",
    "    predicted_classes_per_timestamp = [np.argmax(sublist, axis=1) for sublist in predictions]   \n",
    "    print(\"Predicted classes:\")\n",
    "    for timestamp, predicted_classes in enumerate(predicted_classes_per_timestamp):\n",
    "        print(f\"y_test {timestamp}: {' '.join(map(str, predicted_classes))}\")\n",
    "\n",
    "print(history_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_accuracy:  [0.1111111119389534, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068, 0.2222222238779068]\n",
      "accuracy:  [0.24242424964904785, 0.24242424964904785, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.27272728085517883, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.36363640427589417, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718, 0.3636363744735718]\n",
      "val_loss:  [162.9313201904297, 17.36806297302246, 36.794708251953125, 91.54683685302734, 63.410186767578125, 14.937392234802246, 6.464125156402588, 32.909690856933594, 48.34095764160156, 34.90489196777344, 12.904507637023926, 5.472482681274414, 14.50767993927002, 24.52150535583496, 23.52667236328125, 14.34587574005127, 7.100013256072998, 7.1250224113464355, 11.492825508117676, 14.1528959274292, 12.403775215148926, 8.396102905273438, 5.751981735229492, 6.051023960113525, 7.941442966461182, 8.88222885131836, 7.7001471519470215, 5.558164119720459, 4.476881504058838, 5.130115509033203, 6.246519565582275, 6.2506422996521, 5.0797882080078125, 4.04845666885376, 4.070359230041504, 4.703745365142822, 4.949751377105713, 4.4575300216674805, 3.828282356262207, 3.6855385303497314, 3.9520957469940186, 4.153131008148193, 4.013330936431885, 3.6505367755889893, 3.4901320934295654, 3.588207483291626, 3.7682018280029297, 3.688585042953491, 3.5235092639923096, 3.3415164947509766]\n",
      "loss:  [10.802743911743164, 162.4500732421875, 16.862545013427734, 36.242958068847656, 91.01013946533203, 62.854713439941406, 14.314757347106934, 5.77403450012207, 32.18657302856445, 47.608116149902344, 34.16083908081055, 12.139694213867188, 4.699709892272949, 13.749988555908203, 23.7723445892334, 22.761661529541016, 13.53985595703125, 6.2503275871276855, 6.237826347351074, 10.57568359375, 13.224359512329102, 11.474609375, 7.468667984008789, 4.823461532592773, 5.126180648803711, 7.024867534637451, 7.959236145019531, 6.758738994598389, 4.588632106781006, 3.488682746887207, 4.133414268493652, 5.23911190032959, 5.23160982131958, 4.058862686157227, 3.022096872329712, 3.0417678356170654, 3.6763174533843994, 3.9053146839141846, 3.406118869781494, 2.7707226276397705, 2.603205919265747, 2.858264923095703, 3.0556085109710693, 2.892681121826172, 2.5400474071502686, 2.3499345779418945, 2.4424750804901123, 2.5973448753356934, 2.5422770977020264, 2.3243660926818848]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
