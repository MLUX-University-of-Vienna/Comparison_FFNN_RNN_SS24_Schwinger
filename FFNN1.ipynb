{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import sys\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras_tuner as kt\n",
    "import category_encoders\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# check that correct virtual env is being used\n",
    "#print(sys.executable)\n",
    "print(sys.version)\n",
    "\n",
    "# list all installed modules\n",
    "#%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n#print dataframe row by row\\nfor i in range(len(all_sessions)):\\n    print(all_sessions.loc[i])\\n    break\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def number_of_unique_values_per_column(dataframe: pd.DataFrame) -> None:\n",
    "    print([(column, dataframe[column].nunique()) for column in dataframe.columns])\n",
    "\n",
    "# remove all columns that don't contain any important information, since all entries have the same entry\n",
    "def remove_columns_with_a_single_value(original_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    return original_dataframe[[column for column in all_sessions.columns if len(all_sessions[column].unique()) > 1]]\n",
    "\n",
    "def move_column_to_the_front(original_dataframe: pd.DataFrame, column_to_move: str) -> pd.DataFrame:\n",
    "    first_column = original_dataframe.pop(column_to_move)\n",
    "    original_dataframe.insert(0, column_to_move, first_column)\n",
    "    return original_dataframe\n",
    "\n",
    "def encode_time(original_dataframe: pd.DataFrame, columns_to_encode: list[str]) -> pd.DataFrame:\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    for index in range(0, len(columns_to_encode)):\n",
    "        original_dataframe[columns_to_encode[index]] = original_dataframe[columns_to_encode[index]].astype('float64')\n",
    "        new_df[columns_to_encode[index]] = np.sin(2 * np.pi * original_dataframe[columns_to_encode[index]] / 24.0)\n",
    "    original_dataframe.update(new_df)\n",
    "\n",
    "    return original_dataframe\n",
    "\n",
    "\n",
    "def encode_utc(original_dataframe: pd.DataFrame, columns_to_encode: list[str]) -> pd.DataFrame:\n",
    "    new_df = pd.DataFrame()\n",
    "    pattern = \"%Y-%m-%dT%H:%M:%S.%fZ\"\n",
    "\n",
    "    for index in range(0, len(columns_to_encode)):\n",
    "        if index == 0: # time_utc\n",
    "            pattern = \"%Y-%m-%dT%H:%M:%S.%fZ\"\n",
    "        else: # time_local\n",
    "            pattern = \"%Y-%m-%dT%H:%M:%S.%f\"\n",
    "\n",
    "        timestamps = []\n",
    "        for i in range(len(original_dataframe)):\n",
    "            time_stamp = datetime.strptime(original_dataframe.loc[i, columns_to_encode[index]], pattern)\n",
    "            timestamps.append(time_stamp.timestamp())\n",
    "        new_df[columns_to_encode[index]] = timestamps\n",
    "    \n",
    "    original_dataframe.update(new_df)\n",
    "    \n",
    "    return original_dataframe\n",
    "\n",
    "\n",
    "def binary_encode_ids(original_dataframe: pd.DataFrame, columns_to_encode: list[str]) -> pd.DataFrame:\n",
    "    binary_encoder = category_encoders.BinaryEncoder(cols=columns_to_encode, return_df=True)\n",
    "    original_dataframe = binary_encoder.fit_transform(original_dataframe)\n",
    "    return original_dataframe\n",
    "\n",
    "def encode_boolean(original_dataframe: pd.DataFrame, columns_to_encode: list[str]) -> pd.DataFrame:\n",
    "    for index in range(0, len(columns_to_encode)):\n",
    "        original_dataframe[columns_to_encode[index]] = original_dataframe[columns_to_encode[index]].astype('int')\n",
    "    return original_dataframe\n",
    "\n",
    "def encode_classification(original_dataframe: pd.DataFrame, column_to_encode: str) -> pd.DataFrame:\n",
    "    label_encoder = LabelEncoder()\n",
    "    original_dataframe[column_to_encode] = label_encoder.fit_transform(all_sessions[column_to_encode])\n",
    "    return original_dataframe\n",
    "\n",
    "# read json data\n",
    "with open(\"datasets\\\\transfer\\\\smaller_dataset.json\") as file:\n",
    "    parsed_json = json.load(file)\n",
    "\n",
    "# build dataframe \n",
    "total_amount_of_rows = 0\n",
    "all_sessions = pd.DataFrame()\n",
    "for i in pd.json_normalize(parsed_json['traces']):\n",
    "    total_amount_of_rows += len(pd.json_normalize(parsed_json['traces'][i]))\n",
    "    single_session = pd.json_normalize(parsed_json['traces'][i])\n",
    "    all_sessions = pd.concat([all_sessions, single_session], ignore_index=True)\n",
    "\n",
    "all_sessions = remove_columns_with_a_single_value(all_sessions)\n",
    "\n",
    "# encode 24h time format\n",
    "all_sessions = encode_time(all_sessions, ['time_dow', 'time_hod'])\n",
    "\n",
    "# encode UTC Timestamp\n",
    "all_sessions = encode_utc(all_sessions, ['time_utc', 'time_local'])\n",
    "\n",
    "# encode boolean values to int values\n",
    "all_sessions = encode_boolean(all_sessions, ['device_online'])\n",
    "\n",
    "# binary encode session_id and device_id\n",
    "all_sessions = binary_encode_ids(all_sessions, ['session_id', 'device_id'])\n",
    "\n",
    "# encode the classifcation column \n",
    "all_sessions = encode_classification(all_sessions, 'content_id')\n",
    "\n",
    "# move classification column to the front \n",
    "all_sessions = move_column_to_the_front(all_sessions, 'content_id')\n",
    "\n",
    "# grouped all entries by session_id\n",
    "grouped = all_sessions.groupby(['session_id_0', 'session_id_1', 'session_id_2', 'session_id_3', 'session_id_4', 'session_id_5', 'session_id_6'])\n",
    "\n",
    "# number of columns \n",
    "number_of_columns = all_sessions.shape[1]\n",
    "\n",
    "feature_vectors = []\n",
    "for name, group in grouped:\n",
    "    iterator, classification_iterator = itertools.tee(group.itertuples(index=False))\n",
    "    next(classification_iterator, None) \n",
    "    \n",
    "    prev_prev_feature_vector = ['UNKNOWN'] * number_of_columns\n",
    "    prev_feature_vector = ['UNKNOWN'] * number_of_columns\n",
    "    feature_vector = []\n",
    "    vector_to_add = []\n",
    "    \n",
    "    for row in iterator:\n",
    "        # geginning with the second iteration move all feature_vectors back one step\n",
    "        if prev_feature_vector != []:\n",
    "            prev_prev_feature_vector = prev_feature_vector\n",
    "        \n",
    "        if feature_vector != []:\n",
    "            prev_feature_vector = feature_vector\n",
    "\n",
    "        # build current_feature_vector\n",
    "        feature_vector = [str(element) for element in row]\n",
    "\n",
    "        # get the classifier of the next event for the feature_vector   \n",
    "        classification = 'UNKNOWN'\n",
    "        try:\n",
    "            next_row = next(classification_iterator)\n",
    "            classification = next_row[0] \n",
    "        except StopIteration:\n",
    "            classification = 'UNKNOWN'\n",
    "\n",
    "        # build feature_vector with prev_prev and prev_feature_vector\n",
    "        vector_to_add = [prev_prev_feature_vector, prev_feature_vector, feature_vector]\n",
    "\n",
    "        # append feature_vector to all feature_vectors\n",
    "        feature_vectors.append((vector_to_add, classification))\n",
    "\n",
    "# shuffle the dataset \n",
    "random.shuffle(feature_vectors)\n",
    "\n",
    "data = [([input_features], label) for input_features, label in feature_vectors]\n",
    "\n",
    "input_features = [features for features, _ in data]\n",
    "labels = [label for _, label in data]\n",
    "concatened_input_features = np.concatenate(input_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "''' \n",
    "#print dataframe row by row\n",
    "for i in range(len(all_sessions)):\n",
    "    print(all_sessions.loc[i])\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(3, 18)))\n",
    "\n",
    "    hp_units = hp.Int('units', min_value=16, max_value=512, step=32)\n",
    "    hp_layers = hp.Int('layers', min_value=2, max_value=7, step=1)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    for _ in range(hp_layers):\n",
    "        model.add(tf.keras.layers.Dense(units=hp_units, activation='relu')) \n",
    "\n",
    "    model.add(tf.keras.layers.Dense(highest_value))\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=50,\n",
    "                     directory='with_Unknown',\n",
    "                     project_name='trained_models\\\\first_FNN')\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2)\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))\n",
    "\n",
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "hypermodel.fit(X_train, y_train, epochs=best_epoch, batch_size=32, validation_split=0.2)\n",
    "\n",
    "eval_result = hypermodel.evaluate(X_test, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
