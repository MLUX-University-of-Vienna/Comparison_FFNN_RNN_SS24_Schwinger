{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import sys\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras_tuner as kt\n",
    "import category_encoders\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that correct virtual env is being used\n",
    "#print(sys.executable)\n",
    "print(sys.version)\n",
    "\n",
    "# list all installed modules\n",
    "#%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_unique_values_per_column(dataframe: pd.DataFrame) -> None:\n",
    "    print([(column, dataframe[column].nunique()) for column in dataframe.columns])\n",
    "\n",
    "# remove all columns that don't contain any important information, since all entries have the same entry\n",
    "def remove_columns_with_a_single_value(original_dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    return original_dataframe[[column for column in all_sessions.columns if len(all_sessions[column].unique()) > 1]]\n",
    "\n",
    "def move_column_to_the_front(original_dataframe: pd.DataFrame, column_to_move: str) -> pd.DataFrame:\n",
    "    first_column = original_dataframe.pop(column_to_move)\n",
    "    original_dataframe.insert(0, column_to_move, first_column)\n",
    "    return original_dataframe\n",
    "\n",
    "def encode_time(original_dataframe: pd.DataFrame, columns_to_encode: list[str]) -> pd.DataFrame:\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    for index in range(0, len(columns_to_encode)):\n",
    "        original_dataframe[columns_to_encode[index]] = original_dataframe[columns_to_encode[index]].astype('float64')\n",
    "        new_df[columns_to_encode[index]] = np.sin(2 * np.pi * original_dataframe[columns_to_encode[index]] / 24.0)\n",
    "    original_dataframe.update(new_df)\n",
    "\n",
    "    return original_dataframe\n",
    "\n",
    "def encode_utc(original_dataframe: pd.DataFrame, columns_to_encode: list[str]) -> pd.DataFrame:\n",
    "    new_df = pd.DataFrame()\n",
    "    pattern = \"%Y-%m-%dT%H:%M:%S.%fZ\"\n",
    "\n",
    "    for index in range(0, len(columns_to_encode)):\n",
    "        if index == 0: # time_utc\n",
    "            pattern = \"%Y-%m-%dT%H:%M:%S.%fZ\"\n",
    "        else: # time_local\n",
    "            pattern = \"%Y-%m-%dT%H:%M:%S.%f\"\n",
    "\n",
    "        timestamps = []\n",
    "        for i in range(len(original_dataframe)):\n",
    "            time_stamp = datetime.strptime(original_dataframe.loc[i, columns_to_encode[index]], pattern)\n",
    "            timestamps.append(time_stamp.timestamp())\n",
    "        new_df[columns_to_encode[index]] = timestamps\n",
    "    \n",
    "    original_dataframe.update(new_df)\n",
    "    \n",
    "    return original_dataframe\n",
    "\n",
    "def binary_encode_ids(original_dataframe: pd.DataFrame, columns_to_encode: list[str]) -> pd.DataFrame:\n",
    "    binary_encoder = category_encoders.BinaryEncoder(cols=columns_to_encode, return_df=True)\n",
    "    original_dataframe = binary_encoder.fit_transform(original_dataframe)\n",
    "    return original_dataframe\n",
    "\n",
    "def encode_boolean(original_dataframe: pd.DataFrame, columns_to_encode: list[str]) -> pd.DataFrame:\n",
    "    for index in range(0, len(columns_to_encode)):\n",
    "        original_dataframe[columns_to_encode[index]] = original_dataframe[columns_to_encode[index]].astype('int')\n",
    "    return original_dataframe\n",
    "\n",
    "def encode_classification(original_dataframe: pd.DataFrame, column_to_encode: str) -> pd.DataFrame:\n",
    "    label_encoder = LabelEncoder()\n",
    "    original_dataframe[column_to_encode] = label_encoder.fit_transform(all_sessions[column_to_encode])\n",
    "    return original_dataframe\n",
    "\n",
    "def get_weather_data(original_dataframe: pd.DataFrame, column_to_encode: str, json_subarray_name: str) -> pd.DataFrame:\n",
    "    weather_data_list = []\n",
    "    \n",
    "    for _, row in original_dataframe.iterrows():\n",
    "        weather_day_id = row[column_to_encode]\n",
    "        weather_day_data = parsed_json[json_subarray_name][str(weather_day_id)]\n",
    "        weather_day_data = {k: v for k, v in weather_day_data.items() if k != 'id'}\n",
    "        weather_data_list.append(weather_day_data)\n",
    "    \n",
    "    weather_data_df = pd.DataFrame(weather_data_list)\n",
    "    \n",
    "    original_dataframe = pd.concat([original_dataframe, weather_data_df], axis=1)\n",
    "    \n",
    "    return original_dataframe\n",
    "\n",
    "# read json data\n",
    "with open(\"datasets\\\\transfer\\\\smaller_dataset.json\") as file:\n",
    "    parsed_json = json.load(file)\n",
    "\n",
    "# build dataframe \n",
    "total_amount_of_rows = 0\n",
    "all_sessions = pd.DataFrame()\n",
    "for i in pd.json_normalize(parsed_json['traces']):\n",
    "    total_amount_of_rows += len(pd.json_normalize(parsed_json['traces'][i]))\n",
    "    single_session = pd.json_normalize(parsed_json['traces'][i])\n",
    "    all_sessions = pd.concat([all_sessions, single_session], ignore_index=True)\n",
    "\n",
    "all_sessions = remove_columns_with_a_single_value(all_sessions)\n",
    "\n",
    "# encode 24h time format\n",
    "all_sessions = encode_time(all_sessions, ['time_dow', 'time_hod'])\n",
    "\n",
    "# encode UTC Timestamp\n",
    "all_sessions = encode_utc(all_sessions, ['time_utc', 'time_local'])\n",
    "\n",
    "# encode boolean values to int values\n",
    "all_sessions = encode_boolean(all_sessions, ['device_online'])\n",
    "\n",
    "# binary encode session_id and device_id\n",
    "all_sessions = binary_encode_ids(all_sessions, ['session_id', 'device_id'])\n",
    "\n",
    "# encode the classifcation column \n",
    "all_sessions = encode_classification(all_sessions, 'content_id')\n",
    "\n",
    "# move classification column to the front \n",
    "all_sessions = move_column_to_the_front(all_sessions, 'content_id')\n",
    "\n",
    "all_sessions = get_weather_data(all_sessions, 'weather_day_id', 'weather_day_map')\n",
    "all_sessions = get_weather_data(all_sessions, 'weather_hour_id', 'weather_hour_map')\n",
    "print(len(all_sessions.columns))\n",
    "\n",
    "# grouped all entries by session_id\n",
    "grouped = all_sessions.groupby(['session_id_0', 'session_id_1', 'session_id_2', 'session_id_3', 'session_id_4', 'session_id_5', 'session_id_6'])\n",
    "\n",
    "# number of columns \n",
    "number_of_columns = all_sessions.shape[1]\n",
    "\n",
    "feature_vectors = []\n",
    "for name, group in grouped:\n",
    "    iterator, classification_iterator = itertools.tee(group.itertuples(index=False))\n",
    "    next(classification_iterator, None) \n",
    "    \n",
    "    prev_prev_feature_vector = ['UNKNOWN'] * number_of_columns\n",
    "    prev_feature_vector = ['UNKNOWN'] * number_of_columns\n",
    "    feature_vector = []\n",
    "    vector_to_add = []\n",
    "    \n",
    "    for row in iterator:\n",
    "        # geginning with the second iteration move all feature_vectors back one step\n",
    "        if prev_feature_vector != []:\n",
    "            prev_prev_feature_vector = prev_feature_vector\n",
    "        \n",
    "        if feature_vector != []:\n",
    "            prev_feature_vector = feature_vector\n",
    "\n",
    "        # build current_feature_vector\n",
    "        feature_vector = [str(element) for element in row]\n",
    "\n",
    "        # get the classifier of the next event for the feature_vector   \n",
    "        classification = 'UNKNOWN'\n",
    "        try:\n",
    "            next_row = next(classification_iterator)\n",
    "            classification = next_row[0] \n",
    "        except StopIteration:\n",
    "            classification = 'UNKNOWN'\n",
    "\n",
    "        # build feature_vector with prev_prev and prev_feature_vector\n",
    "        vector_to_add = [prev_prev_feature_vector, prev_feature_vector, feature_vector]\n",
    "\n",
    "        # append feature_vector to all feature_vectors\n",
    "        feature_vectors.append((vector_to_add, classification))\n",
    "\n",
    "# shuffle the dataset \n",
    "random.shuffle(feature_vectors)\n",
    "\n",
    "\n",
    "''' \n",
    "for i in reversed(range(len(feature_vectors))):\n",
    "    feature = feature_vectors[i]\n",
    "    if 'UNKNOWN' in feature[0][0] or 'UNKNOWN' in feature[0][1] or 'UNKNOWN' in feature[0][2] or isinstance(feature[1], str):\n",
    "        del feature_vectors[i]\n",
    "    elif 'None' in feature[0][0] or 'None' in feature[0][1] or 'None' in feature[0][2]:\n",
    "        del feature_vectors[i]    \n",
    "\n",
    "\n",
    "for i in range(len(feature_vectors)):\n",
    "    feature_vector = feature_vectors[i][0]\n",
    "    classification = feature_vectors[i][1]\n",
    "    feature_vector = [[float(element) for element in sublist] for sublist in feature_vector]\n",
    "    feature_vectors[i] = (feature_vector, classification)\n",
    "\n",
    "data = [([input_features], label) for input_features, label in feature_vectors]\n",
    "\n",
    "input_features = [features for features, _ in data]\n",
    "labels = [label for _, label in data]\n",
    "concatened_input_features = np.concatenate(input_features)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_features, labels, test_size=0.2, random_state=42)\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "y_train = np.asarray(y_train)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "\n",
    "#print dataframe row by row\n",
    "for i in range(len(all_sessions)):\n",
    "    print(all_sessions.loc[i])\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
